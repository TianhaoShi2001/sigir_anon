{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_df_origin = pd.read_pickle('./small_Books.pkl')\n",
    "meta_df_origin = pd.read_pickle('./small_meta_Books.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_df = book_df_origin.copy()\n",
    "meta_df = meta_df_origin.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(meta_df.columns)\n",
    "print(book_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. seperate cold-start and warm-start items according to item publish time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_title2id_dict = {}\n",
    "id_num = 0 \n",
    "unique_titles = meta_df['title'].unique().tolist()\n",
    "for title in unique_titles:\n",
    "    original_title2id_dict[title] = id_num\n",
    "    id_num+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df['title_idx'] = meta_df['title'].apply(lambda x:original_title2id_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(book_df.columns, meta_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_df['timestamp'] = pd.to_datetime(book_df['timestamp'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_meta_df = meta_df.copy()[['title','parent_asin','title_idx','publish_time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_book_df = pd.merge(left = book_df, right=partial_meta_df,how = 'left',on='parent_asin', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_book_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_book_df['publish_time'].replace('\\\\N', np.nan, inplace=True)\n",
    "merged_book_df.dropna(inplace=True)\n",
    "merged_book_df['publish_time'] = merged_book_df['publish_time'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = merged_book_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['uid',  'iid','rating', 'timestamp', 'title', 'title_idx','publish_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_publist_earliest_time = data.groupby('title_idx').agg({'publish_time':'min'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(item_publist_earliest_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cold_publish_data = item_publist_earliest_time[item_publist_earliest_time['publish_time'] >= 2023]\n",
    "cold_publish_items = cold_publish_data.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cold_data = data[data['title_idx'].isin(cold_publish_items)]\n",
    "print(cold_data['timestamp'].min())\n",
    "print(cold_data['publish_time'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_review_earliest_time = data.groupby('title_idx').agg({'timestamp':'min'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cold_review_data = item_review_earliest_time[item_review_earliest_time['timestamp'] >= pd.to_datetime('2022-10-01')]\n",
    "cold_review_items = cold_review_data.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cold_publish_items))\n",
    "print(len(cold_review_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cold_items = set(cold_review_items).intersection(set(cold_publish_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cold_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.get warm_train, warm_test, cold_test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warm_start_time = '2020-01-01'\n",
    "cold_start_time = '2023-01-01'\n",
    "cold_end_time = '2023-10-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = merged_book_df.copy()\n",
    "data.columns = ['uid',  'iid','rating', 'timestamp', 'title', 'title_idx','publish_time']\n",
    "cold_data = data[data['title_idx'].isin(cold_items)]\n",
    "uncold_data = data[~data['title_idx'].isin(cold_items)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete cold data' interactions\n",
    "cold_data_test = cold_data[(cold_data['timestamp'] >= pd.to_datetime(cold_start_time)) & (cold_data['timestamp'] <= pd.to_datetime(cold_end_time))]\n",
    "cold_items_test = cold_data_test['title_idx'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_items = uncold_data['iid'].unique().tolist()\n",
    "preserve_warm_items = list((set(old_items)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preserve_warm_data = uncold_data[uncold_data['iid'].isin(preserve_warm_items)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process only warm_data\n",
    "preserve_warm_data_test_index = (preserve_warm_data['timestamp'] >= pd.to_datetime(cold_start_time)) & (preserve_warm_data['timestamp'] <= pd.to_datetime(cold_end_time))\n",
    "preserve_warm_data_train_index= (preserve_warm_data['timestamp'] >= pd.to_datetime(warm_start_time)) & (preserve_warm_data['timestamp'] < pd.to_datetime(cold_start_time))\n",
    "preserve_warm_data_test = preserve_warm_data[preserve_warm_data_test_index]\n",
    "preserve_warm_data_train = preserve_warm_data[preserve_warm_data_train_index]\n",
    "print(preserve_warm_data_train.shape[0])\n",
    "print(preserve_warm_data_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preserve_warm_data_train['env'] = 0 # warm train\n",
    "preserve_warm_data_test['env'] = 1 # warm test\n",
    "cold_data_test['env'] = 3 # cold\n",
    "all_data = pd.concat([preserve_warm_data_train,preserve_warm_data_test,cold_data_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_users(all_data):\n",
    "    user_cnt = all_data['uid'].value_counts()\n",
    "    warm_users = user_cnt[user_cnt >= 50].index.tolist()\n",
    "    all_data = all_data[all_data['uid'].isin(warm_users)]\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filer_items(all_data):\n",
    "    item_cnt = all_data['iid'].value_counts()\n",
    "    filer_items = item_cnt[item_cnt >=50].index.tolist()\n",
    "    all_data = all_data[all_data['iid'].isin(filer_items)]\n",
    "\n",
    "    # filter items again, to make sure the warm items in the test set are warm---delete items with less than 10 inters in the training set\n",
    "    train_warm_cnt = all_data[all_data['env'] == 0]['iid'].value_counts()\n",
    "    delete_items = train_warm_cnt[train_warm_cnt < 50].index.unique().tolist()\n",
    "    all_data = all_data[~all_data['iid'].isin(delete_items)]\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_data.shape[0])\n",
    "for i in range(5):\n",
    "    all_data = filter_users(all_data)\n",
    "    all_data = filer_items(all_data)\n",
    "    print(all_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_data.shape[0])\n",
    "print('warm train inter num',all_data[all_data['env'] == 0].shape[0]) # warm_train\n",
    "print('warm evaluate inter num',all_data[all_data['env'] == 1].shape[0]) # warm_test\n",
    "print('cold evaluate inter num',all_data[all_data['env'] == 3].shape[0]) # cold (cold_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cold_data = all_data[all_data['env'] == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = cold_data['iid'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.save data for TALLRec and CF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parent_asin to iid\n",
    "parent_asin2iid_dict = {}\n",
    "num = 0\n",
    "for parent_asin in all_data['iid'].unique().tolist():\n",
    "    parent_asin2iid_dict[parent_asin] = num\n",
    "    num += 1\n",
    "all_data['iid'] = all_data['iid'].apply(lambda x: parent_asin2iid_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(parent_asin2iid_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parent_asin to iid\n",
    "user_id2uid_dict = {}\n",
    "num = 0\n",
    "for user_id in all_data['uid'].unique().tolist():\n",
    "    user_id2uid_dict[user_id] = num\n",
    "    num += 1\n",
    "all_data['uid'] = all_data['uid'].apply(lambda x: user_id2uid_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(user_id2uid_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_data.columns)\n",
    "print(all_data['publish_time'].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.sort_values(by = 'timestamp', inplace=True, ascending=True)\n",
    "all_data['label'] = all_data['rating'].apply(lambda x: 1 if x>=4 else 0)\n",
    "all_data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_inter_all = all_data.groupby('uid').agg({'iid':list, 'label':list, 'title':list, 'timestamp':list, 'env':list})\n",
    "print(u_inter_all.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def deal_with_each_u(x,u):\n",
    "    items = np.array(x.iid)\n",
    "    labels = np.array(x.label)\n",
    "    titles = np.array(x.title)\n",
    "    timestamp = np.array(x.timestamp)\n",
    "    env = np.array(x.env)\n",
    "    his = [0] # adding a '0' by default\n",
    "    his_title = ['']\n",
    "    results = []\n",
    "    for i in range(items.shape[0]):\n",
    "        results.append((u, items[i], timestamp[i], np.array(his), copy.copy(his_title),titles[i], labels[i],env[i]))\n",
    "        # training data\n",
    "        if labels[i] > 0: # positive \n",
    "            his.append(items[i])\n",
    "            his_title.append(titles[i])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for u in u_inter_all.index:\n",
    "    results.extend(deal_with_each_u(u_inter_all.loc[u],u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_, i_, time_, label_, his_, his_title, title_,env_ = [],[],[],[],[],[],[],[]\n",
    "for re_ in results:\n",
    "        u_.append(re_[0])\n",
    "        i_.append(re_[1])\n",
    "        time_.append(re_[2])\n",
    "        his_.append(re_[3][-15:])\n",
    "        his_title.append(re_[4][-15:])\n",
    "        title_.append(re_[5])\n",
    "        label_.append(re_[6])\n",
    "        env_.append(re_[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.DataFrame({\"uid\":u_,'iid':i_,'label':label_, 'timestamp': time_ , 'his':his_,'his_title':his_title,'title':title_,'env':env_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_data.shape)\n",
    "all_data.sort_values(by = 'timestamp', ascending=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def under_sample(df, p):\n",
    "    items = df['iid'].unique().tolist()\n",
    "    random.seed(2024)\n",
    "    random.shuffle(items)\n",
    "    preserve_items = items[ : int(len(items) * p)]\n",
    "    new_df = df[df['iid'].isin(preserve_items)]\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = all_data[all_data['env'] == 0]\n",
    "warm_test = all_data[all_data['env'] == 1]\n",
    "cold_test = all_data[all_data['env'] == 3]\n",
    "\n",
    "# Under-sample evaluation data for warm items to make their quantity close to that of cold items,\n",
    "# as we use the metrics from the mixed validation set to early stop, following previous work.\n",
    "warm_test = under_sample(warm_test, p=0.02)\n",
    "\n",
    "print('warm train inter num', train_data.shape[0])  # warm_train\n",
    "print('warm evaluate inter num', warm_test.shape[0])  # warm_test\n",
    "print('cold evaluate inter num', cold_test.shape[0])  # cold (cold_test)\n",
    "mix_test = pd.concat((warm_test, cold_test))\n",
    "unique_iids = mix_test['iid'].unique().tolist()\n",
    "valid_data = []\n",
    "test_data = []\n",
    "for iid in unique_iids:\n",
    "    iid_data = mix_test.loc[mix_test['iid'] == iid]\n",
    "    if iid_data['env'].iloc[0] == 1:  # For warm items\n",
    "        split_index_valid = len(iid_data) // 2\n",
    "        valid_data.append(iid_data.iloc[:split_index_valid])  \n",
    "        test_data.append(iid_data.iloc[split_index_valid:])  \n",
    "    elif iid_data['env'].iloc[0] == 3:  # For cold items\n",
    "        split_index_valid = len(iid_data) // 4\n",
    "        split_index_test = len(iid_data) // 2\n",
    "        valid_data.append(iid_data.iloc[:split_index_valid])  \n",
    "        test_data.append(iid_data.iloc[split_index_valid:split_index_test])  \n",
    "valid_all = pd.concat(valid_data)\n",
    "test_all = pd.concat(test_data)\n",
    "\n",
    "# Separate warm and cold validation and test sets\n",
    "warm_valid = valid_all[valid_all['env'] == 1]\n",
    "cold_valid = valid_all[valid_all['env'] == 3]\n",
    "warm_test = test_all[test_all['env'] == 1]\n",
    "cold_test = test_all[test_all['env'] == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_pickle('./train0.pkl')  # train_warm\n",
    "valid_all.to_pickle('./valid0.pkl') # valid_mix\n",
    "test_all.to_pickle('./test0.pkl') # valid_mix\n",
    "warm_valid.to_pickle('./valid1.pkl') # valid_warm\n",
    "cold_valid.to_pickle('./valid2.pkl') # valid_cold\n",
    "warm_test.to_pickle('./test1.pkl') # test_warm\n",
    "cold_test.to_pickle('./test2.pkl')  # test_cold\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "song1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
